name: SOFIA
base_model: Qwen/Qwen3-Embedding-8B
output_dir: ./SOFIA
seed: 42
epochs: 2
batch_size: 128
lr: 2.0e-5
warmup_ratio: 0.06
max_len: 512
fp16: true
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
kd:
  teachers:
    - BAAI/bge-m3
    - intfloat/e5-mistral-7b-instruct
  kd_weight: 0.6
losses:
  - cosine
  - triplet
instruction_templates:
  query: "Query: {text}"
  doc:   "Document: {text}"
dims_to_export: [1024, 3072, 4096]
datasets:
  stsb: sentence-transformers/stsb
  nq:   mteb/nq
  quora: paws
  paws: paws
  banking77: banking77
hard_negatives:
  bm25_k: 50
  faiss_k: 50
  per_pos: 2
